{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of the Full Project Pipeline**\n",
        "- Input: Raw RGB image.\n",
        "- Opponency: Break into Intensity, Red-Green, and Blue-Yellow.\n",
        "- Center-Surround: Apply Difference of Gaussians (DoG) at multiple scales.\n",
        "- Integration: Combine scales into a master Saliency Map.\n",
        "- Gating: Mask the original image to create a \"Foveal\" view.\n",
        "- Classification: Predict the object based on the attended pixels."
      ],
      "metadata": {
        "id": "9G4tC_vfa-kg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Concept: Difference of Gaussians (DoG)\n",
        "\n",
        "To mimic the \"On-Center/Off-Surround\" cell, we calculate the difference between a narrow Gaussian kernel (the center) and a wide Gaussian kernel (the surround)."
      ],
      "metadata": {
        "id": "jzKVJyRSRCmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementation\n",
        "\n",
        "We will define a `CenterSurroundAttention` layer. Instead of standard \"dot-product\" attention, this uses spatial filtering to determine what the model should focus on."
      ],
      "metadata": {
        "id": "RqljnXXLRMRI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX70hlT9Q2ta"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CenterSurroundLayer(nn.Module):\n",
        "    def __init__(self, channels, kernel_size=7, sigma_center=0.5, sigma_surround=3.0):\n",
        "        super(CenterSurroundLayer, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Create a coordinate grid\n",
        "        x = torch.arange(kernel_size) - (kernel_size - 1) / 2\n",
        "        y = torch.arange(kernel_size) - (kernel_size - 1) / 2\n",
        "        grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n",
        "        dist_sq = grid_x**2 + grid_y**2\n",
        "\n",
        "        # Calculate Gaussian kernels\n",
        "        center = torch.exp(-dist_sq / (2 * sigma_center**2))\n",
        "        surround = torch.exp(-dist_sq / (2 * sigma_surround**2))\n",
        "\n",
        "        # Center-Surround (On-Center / Off-Surround)\n",
        "        # Normalizing ensures we don't explode the brightness\n",
        "        dog_kernel = (center / center.sum()) - (surround / surround.sum())\n",
        "\n",
        "        # Reshape for depthwise convolution: [out_channels, in_channels/groups, H, W]\n",
        "        dog_kernel = dog_kernel.view(1, 1, kernel_size, kernel_size)\n",
        "        self.register_buffer('weight', dog_kernel.repeat(channels, 1, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Using functional conv2d with groups=channels for depthwise operation\n",
        "        return F.conv2d(x, self.weight, padding=self.kernel_size//2, groups=self.channels)"
      ],
      "metadata": {
        "id": "MmbsZzbvReDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage:\n",
        "# model = CenterSurroundLayer(channels=3)\n",
        "# output = model(input_tensor)"
      ],
      "metadata": {
        "id": "yiMr55yERmAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why this matters in AI**\n",
        "\n",
        "    Saliency: This layer naturally highlights \"blobs\" and edges, acting as a pre-processor for higher-level attention.\n",
        "\n",
        "    Data Efficiency: Unlike Transformers, which have to learn spatial relationships, this layer enforces a biological prior about how light and shapes work.\n",
        "\n",
        "    Noise Reduction: By subtracting the surround, you effectively perform high-pass filtering, removing low-frequency noise."
      ],
      "metadata": {
        "id": "tqYrjAJVRm5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization in notebook**\n",
        "\n",
        "Since you are in a notebook, you should definitely visualize the kernel to ensure it looks like the \"Mexican Hat\" function."
      ],
      "metadata": {
        "id": "SY_j9qTgRtGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = CenterSurroundLayer(channels=1, kernel_size=21, sigma_center=2.0, sigma_surround=5.0)\n",
        "kernel = layer.weight.squeeze().cpu().numpy()\n",
        "\n",
        "plt.imshow(kernel, cmap='RdBu')\n",
        "plt.colorbar()\n",
        "plt.title(\"Center-Surround Receptive Field (DoG)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tb-eroVWSBkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hard-coded version is great for understanding, but in a hybrid AI model, we usually make `sigma_center` and `sigma_surround` learnable parameters."
      ],
      "metadata": {
        "id": "LnEXOP9USj4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the parameters learnable transitions this from a fixed image processing filter to an Adaptive Center-Surround Attention mechanism. This allows the network to dynamically \"zoom\" its focus based on the scale of the features it's trying to detect.\n",
        "\n",
        "In PyTorch, we can't just make the kernel weights learnable if we want to maintain the Gaussian shape. Instead, we make the σ (variance) values learnable. This ensures the model always follows biological constraints while optimizing for performance."
      ],
      "metadata": {
        "id": "4-rXEG6eSz5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adaptive Model"
      ],
      "metadata": {
        "id": "_4i9hHhUS3_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaptiveCenterSurround(nn.Module):\n",
        "    def __init__(self, channels, kernel_size=15):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Initialize sigmas as learnable parameters\n",
        "        # We use log-space or softplus to ensure they stay positive\n",
        "        self.sigma_center = nn.Parameter(torch.tensor([1.0]))\n",
        "        self.sigma_surround = nn.Parameter(torch.tensor([3.0]))\n",
        "\n",
        "        # Pre-calculate coordinate grid\n",
        "        x = torch.arange(kernel_size).float() - (kernel_size - 1) / 2\n",
        "        y = torch.arange(kernel_size).float() - (kernel_size - 1) / 2\n",
        "        grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n",
        "        self.register_buffer('dist_sq', grid_x**2 + grid_y**2)\n",
        "\n",
        "    def get_kernel(self):\n",
        "        # Ensure sigmas are positive and surround > center\n",
        "        s_c = torch.clamp(self.sigma_center, min=0.5)\n",
        "        s_s = torch.clamp(self.sigma_surround, min=s_c.item() + 0.5)\n",
        "\n",
        "        # Generate Gaussians\n",
        "        center = torch.exp(-self.dist_sq / (2 * s_c**2))\n",
        "        surround = torch.exp(-self.dist_sq / (2 * s_s**2))\n",
        "\n",
        "        # Normalize and subtract\n",
        "        dog_kernel = (center / center.sum()) - (surround / surround.sum())\n",
        "        return dog_kernel.view(1, 1, self.kernel_size, self.kernel_size).repeat(self.channels, 1, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        kernel = self.get_kernel()\n",
        "        return F.conv2d(x, kernel, padding=self.kernel_size//2, groups=self.channels)"
      ],
      "metadata": {
        "id": "a8MXZ6dYSjf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why this Matters?\n",
        "\n",
        "In the primary visual cortex (V1), neurons don't have a fixed resolution. Some cells are tuned to high-frequency edges (small $σ$), while others look at broader textures (large $σ$).\n",
        "\n",
        "By using the code above:\n",
        "\n",
        "1. Backpropagation will now calculate the gradient of the loss with respect to $σ$.\n",
        "2. If the model needs more detail to reduce loss, $σ$ center​ will shrink.\n",
        "3. If the model needs to ignore high-frequency noise, $σ$ surround​ will expand."
      ],
      "metadata": {
        "id": "ygm41tICTEty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the \"Attention\" Effect\n",
        "\n",
        "You can run this simple test in your notebook to see how the model reacts to a simple input (like a white square on a black background):"
      ],
      "metadata": {
        "id": "aQtng2RnTir6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dummy image (black with a white square)\n",
        "img = torch.zeros(1, 1, 32, 32)\n",
        "# change this variable to change the image seeing the attention change\n",
        "img[:, :, 10:22, 10:22] = 1.0\n",
        "\n",
        "model = AdaptiveCenterSurround(channels=1, kernel_size=15)\n",
        "output = model(img)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(img[0,0], cmap='gray')\n",
        "ax[0].set_title(\"Input (Stimulus)\")\n",
        "ax[1].imshow(output.detach()[0,0], cmap='magma')\n",
        "ax[1].set_title(\"Neural Response (Edges)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mhpXL2A-Tncx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taking to Next Step\n",
        "\n",
        "We will be modelling the full pipeline!!\n",
        "\n",
        "We’re essentially recreating the Itti-Koch saliency model, which is the gold standard for biologically inspired computer vision. It simulates how the human brain processes \"bottom-up\" visual attention using multi-scale features."
      ],
      "metadata": {
        "id": "JLq8zItvWuyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: The Multi-Scale Saliency Model\n",
        "\n",
        "The Itti-Koch model works by creating \"Feature Maps\" (Intensity, Color, Orientation) and then applying center-surround operations across different spatial scales."
      ],
      "metadata": {
        "id": "PsYvLK2rWxac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SaliencyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # We use a Gaussian Pyramid approach for multi-scale processing\n",
        "        self.cs_layer = AdaptiveCenterSurround(channels=1, kernel_size=15)\n",
        "\n",
        "    def get_pyramid(self, x, levels=4):\n",
        "        pyramid = [x]\n",
        "        for i in range(levels - 1):\n",
        "            pyramid.append(F.avg_pool2d(pyramid[-1], kernel_size=2, stride=2))\n",
        "        return pyramid\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Convert to Intensity (Greyscale)\n",
        "        intensity = torch.mean(x, dim=1, keepdim=True)\n",
        "\n",
        "        # 2. Build Pyramid (Multi-scale)\n",
        "        scales = self.get_pyramid(intensity)\n",
        "\n",
        "        # 3. Apply Center-Surround to each scale\n",
        "        maps = []\n",
        "        for s in scales:\n",
        "            cs_map = self.cs_layer(s)\n",
        "            # Resize back to original size to combine\n",
        "            maps.append(F.interpolate(cs_map, size=(x.shape[2], x.shape[3]), mode='bilinear'))\n",
        "\n",
        "        # 4. Aggregate Saliency (Summing the maps)\n",
        "        saliency_map = torch.mean(torch.stack(maps), dim=0)\n",
        "        return torch.abs(saliency_map) # Focus on magnitude of contrast"
      ],
      "metadata": {
        "id": "Jsf4gJRdW11q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Heatmap Visualization Tool\n",
        "\n",
        "To make this useful, we need a way to overlay the \"neural firing\" (saliency) onto the original image. This helps us see what the model \"thinks\" is important."
      ],
      "metadata": {
        "id": "ieTStrovXOAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_attention(img_path, model):\n",
        "    # Load and Preprocess\n",
        "    raw_img = Image.open(img_path).convert('RGB')\n",
        "    transform = T.Compose([T.Resize((256, 256)), T.ToTensor()])\n",
        "    input_tensor = transform(raw_img).unsqueeze(0)\n",
        "\n",
        "    # Generate Saliency\n",
        "    with torch.no_grad():\n",
        "        saliency = model(input_tensor)\n",
        "\n",
        "    # Normalize for visualization\n",
        "    s_map = saliency[0, 0].cpu().numpy()\n",
        "    s_map = (s_map - s_map.min()) / (s_map.max() - s_map.min())\n",
        "\n",
        "    # Create Heatmap Overlay\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(raw_img.resize((256, 256)))\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(s_map, cmap='jet')\n",
        "    plt.title(\"Saliency Map\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    # Overlaying\n",
        "    plt.imshow(raw_img.resize((256, 256)))\n",
        "    plt.imshow(s_map, cmap='jet', alpha=0.5) # Alpha blending\n",
        "    plt.title(\"Attention Overlay\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Nm_LKFY8Wj2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the Mechanism\n",
        "\n",
        "The \"Center-Surround\" creates a \"Mexican Hat\" response. In an image, this means the model ignores flat areas (like a clear blue sky) and reacts strongly to discontinuities (like a bird in that sky)."
      ],
      "metadata": {
        "id": "8YVyJfZiXYnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the project"
      ],
      "metadata": {
        "id": "yIZ3H06eX4A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "model = SaliencyModel()\n",
        "\n",
        "# Run on an image (Ensure you have a sample.jpg in your directory)\n",
        "visualize_attention('sample.jpg', model)"
      ],
      "metadata": {
        "id": "5yiRPiM6XcTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving the model"
      ],
      "metadata": {
        "id": "ZXArZXoOYM3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Color Opponency is where the neuroscience really shines. In the primate visual system, the Parvocellular pathway processes \"Red vs. Green\" and \"Blue vs. Yellow\" signals. This is why a red apple pops out against green leaves—not necessarily because it's brighter, but because of the chromatic contrast.\n",
        "\n",
        "Let’s build the final, complete notebook structure."
      ],
      "metadata": {
        "id": "zHgpLJiVYUVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Color Opponency Module**\n",
        "\n",
        "We will create four channels: R (Red), G (Green), B (Blue), and Y (Yellow).\n",
        "\n",
        "- $RG=∣R−G∣$\n",
        "- $BY=∣B−Y∣$ (where $Y=(R+G)\\div2​$)"
      ],
      "metadata": {
        "id": "Tnc4nAEGYViy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorOpponency(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [B, 3, H, W]\n",
        "        r, g, b = x[:, 0, :, :], x[:, 1, :, :], x[:, 2, :, :]\n",
        "\n",
        "        # Intensity (I)\n",
        "        intensity = (r + g + b) / 3\n",
        "\n",
        "        # Color channels normalized by intensity to get pure hue\n",
        "        # We add a small epsilon to avoid division by zero\n",
        "        eps = 1e-5\n",
        "        R = r - (g + b) / 2\n",
        "        G = g - (r + b) / 2\n",
        "        B = b - (r + g) / 2\n",
        "        Y = (r + g) / 2 - torch.abs(r - g) / 2 - b\n",
        "\n",
        "        # Primate-like Opponency Maps\n",
        "        RG = torch.abs(R - G).unsqueeze(1)\n",
        "        BY = torch.abs(B - Y).unsqueeze(1)\n",
        "\n",
        "        return intensity.unsqueeze(1), RG, BY"
      ],
      "metadata": {
        "id": "2G7M3VCVYRZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Integrated Itti-Koch Model**\n",
        "\n",
        "Now we combine Intensity, Color, and Center-Surround across multiple scales."
      ],
      "metadata": {
        "id": "0uFXl-sVY1ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FullIttiKochModel(nn.Module):\n",
        "    def __init__(self, kernel_size=15):\n",
        "        super().__init__()\n",
        "        self.color_engine = ColorOpponency()\n",
        "        # Shared center-surround layer\n",
        "        self.cs_layer = AdaptiveCenterSurround(channels=1, kernel_size=kernel_size)\n",
        "\n",
        "    def process_feature(self, feature_map, levels=4):\n",
        "        # Create pyramid\n",
        "        pyramid = [feature_map]\n",
        "        for _ in range(levels - 1):\n",
        "            pyramid.append(F.avg_pool2d(pyramid[-1], kernel_size=2, stride=2))\n",
        "\n",
        "        # Apply Center-Surround and upscale back\n",
        "        results = []\n",
        "        for p in pyramid:\n",
        "            out = self.cs_layer(p)\n",
        "            results.append(F.interpolate(out, size=(feature_map.shape[2], feature_map.shape[3]), mode='bilinear'))\n",
        "\n",
        "        return torch.mean(torch.stack(results), dim=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        I, RG, BY = self.color_engine(x)\n",
        "\n",
        "        # Extract saliency for each pathway\n",
        "        saliency_I = self.process_feature(I)\n",
        "        saliency_RG = self.process_feature(RG)\n",
        "        saliency_BY = self.process_feature(BY)\n",
        "\n",
        "        # Combine maps (Linear Integration)\n",
        "        combined_saliency = (saliency_I + saliency_RG + saliency_BY) / 3\n",
        "        return torch.abs(combined_saliency)"
      ],
      "metadata": {
        "id": "LpPW1aNBY2nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Visualization Tool (Final Form)**\n",
        "\n",
        "This function will generate the heatmap and overlay it, creating that \"thermal vision\" effect seen in eye-tracking studies."
      ],
      "metadata": {
        "id": "1klEaoA4Y6zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def generate_heatmap(model, image_tensor, original_img_size):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        saliency = model(image_tensor)\n",
        "\n",
        "    # Process saliency map for heatmap\n",
        "    s_map = saliency[0, 0].cpu().numpy()\n",
        "    s_map = (s_map - s_map.min()) / (s_map.max() - s_map.min() + 1e-8)\n",
        "    s_map = (s_map * 255).astype(np.uint8)\n",
        "\n",
        "    # Resize to original image dimensions\n",
        "    s_map_resized = cv2.resize(s_map, original_img_size)\n",
        "\n",
        "    # Apply Color Map (Jet)\n",
        "    heatmap = cv2.applyColorMap(s_map_resized, cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return heatmap, s_map_resized\n",
        "\n",
        "def plot_final_results(img_path):\n",
        "    # Load\n",
        "    raw_img = np.array(Image.open(img_path).convert('RGB'))\n",
        "    h, w, _ = raw_img.shape\n",
        "\n",
        "    # Prep tensor\n",
        "    transform = T.Compose([T.ToTensor(), T.Resize((256, 256))])\n",
        "    input_tensor = transform(Image.fromarray(raw_img)).unsqueeze(0)\n",
        "\n",
        "    # Run Model\n",
        "    model = FullIttiKochModel()\n",
        "    heatmap, gray_saliency = generate_heatmap(model, input_tensor, (w, h))\n",
        "\n",
        "    # Overlay\n",
        "    overlay = cv2.addWeighted(raw_img, 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "    # Display\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    titles = ['Original', 'Saliency (Grey)', 'Heatmap', 'Attention Overlay']\n",
        "    imgs = [raw_img, gray_saliency, heatmap, overlay]\n",
        "\n",
        "    for i in range(4):\n",
        "        plt.subplot(1, 4, i+1)\n",
        "        plt.imshow(imgs[i], cmap='gray' if i==1 else None)\n",
        "        plt.title(titles[i])\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1Q0Ff1lnY9Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Code\n",
        "\n",
        "1. Bio-Plausible Filtering: Using Difference of Gaussians to simulate retinal ganglion cells.\n",
        "2. Learnable Parameters: Allowing σ to be optimized via backprop (if you choose to train it).\n",
        "3. Multi-Scale Processing: Using a Gaussian pyramid to find salient objects of different sizes.\n",
        "4. Chromatic Opponency: Modeling the R/G and B/Y pathways of the human brain."
      ],
      "metadata": {
        "id": "CxqrzzzpZMCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running on a specific dataset(CIFAR) to see how it can be used as a \"foveal\" pre-processor to reduce the amount of data a neural network has to \"look\" at."
      ],
      "metadata": {
        "id": "M8S2zLy5ZVb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Attention-Gated Classifier\n",
        "\n",
        "We will create a wrapper that takes an image, generates the Itti-Koch saliency map, and uses that map to \"gate\" the pixels before they reach a standard CNN (like a simple ResNet or ConvNet)."
      ],
      "metadata": {
        "id": "6uJuGclqZ2Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FovealAttentionNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.saliency_extractor = FullIttiKochModel()\n",
        "\n",
        "        # A simple CNN \"Brain\" to process the gated image\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Generate the attention mask [B, 1, H, W]\n",
        "        mask = self.saliency_extractor(x)\n",
        "\n",
        "        # Normalize mask to [0, 1] range for gating\n",
        "        mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-8)\n",
        "\n",
        "        # 2. \"Foveation\": Multiply input by mask\n",
        "        # This effectively 'blacks out' non-salient areas\n",
        "        gated_x = x * mask\n",
        "\n",
        "        # 3. Classify based ONLY on attended regions\n",
        "        logits = self.classifier(gated_x)\n",
        "        return logits, gated_x, mask"
      ],
      "metadata": {
        "id": "JbDWPhx-Zz3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on CIFAR-10\n",
        "\n",
        "Since CIFAR-10 images are small (32x32), the center-surround effect is very dramatic."
      ],
      "metadata": {
        "id": "XFGDCy7EZ-cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load CIFAR-10\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Initialize Model\n",
        "net = FovealAttentionNet(num_classes=10)\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Run Inference\n",
        "logits, gated_images, masks = net(images)\n",
        "\n",
        "# Visualize the 'Foveation' process\n",
        "fig, axes = plt.subplots(4, 3, figsize=(10, 12))\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "for i in range(4):\n",
        "    # Original\n",
        "    axes[i, 0].imshow(images[i].permute(1, 2, 0))\n",
        "    axes[i, 0].set_title(f\"Original: {classes[labels[i]]}\")\n",
        "\n",
        "    # Mask\n",
        "    axes[i, 1].imshow(masks[i, 0].detach().numpy(), cmap='gray')\n",
        "    axes[i, 1].set_title(\"Saliency Mask\")\n",
        "\n",
        "    # Gated Input\n",
        "    axes[i, 2].imshow(gated_images[i].detach().permute(1, 2, 0))\n",
        "    axes[i, 2].set_title(\"What the Model 'Sees'\")\n",
        "\n",
        "    for ax in axes[i]: ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xolun43FaAa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Background Suppression: If there's a bird on a branch, the leaves and sky are dimmed, and the high-contrast edges of the bird are highlighted.\n",
        "- Information Bottleneck: The classifier is now forced to be sparse. It cannot rely on background correlations (like \"green pixels usually mean deer\") because the attention mechanism filters them out."
      ],
      "metadata": {
        "id": "OqN0QStKabu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparison Training Loop\n",
        "\n",
        "To see if this \"Bio-Plausible\" approach actually helps, we can compare it against a vanilla CNN. We will track the Validation Accuracy for both to see if the attention-gated model learns more efficiently or handles noise better."
      ],
      "metadata": {
        "id": "NRqTWB5tbcjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_and_compare(epochs=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1. Initialize two models\n",
        "    # Standard CNN (no attention)\n",
        "    standard_net = FovealAttentionNet().to(device)\n",
        "    # We \"turn off\" the gating by overriding the forward for comparison\n",
        "    def standard_forward(x):\n",
        "        logits = standard_net.classifier(x)\n",
        "        return logits\n",
        "\n",
        "    # Attention-Gated CNN\n",
        "    attention_net = FovealAttentionNet().to(device)\n",
        "\n",
        "    # 2. Setup Optimizers\n",
        "    opt_std = optim.Adam(standard_net.parameters(), lr=0.001)\n",
        "    opt_attn = optim.Adam(attention_net.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'std_acc': [], 'attn_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # --- Training Loop ---\n",
        "        standard_net.train()\n",
        "        attention_net.train()\n",
        "\n",
        "        for i, (imgs, labels) in enumerate(testloader): # Using small subset for demo\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            # Train Standard\n",
        "            opt_std.zero_grad()\n",
        "            out_std = standard_forward(imgs)\n",
        "            loss_std = criterion(out_std, labels)\n",
        "            loss_std.backward()\n",
        "            opt_std.step()\n",
        "\n",
        "            # Train Attention\n",
        "            opt_attn.zero_grad()\n",
        "            out_attn, _, _ = attention_net(imgs)\n",
        "            loss_attn = criterion(out_attn, labels)\n",
        "            loss_attn.backward()\n",
        "            opt_attn.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} complete.\")\n",
        "        # (In a real scenario, you'd calculate accuracy here and append to history)\n",
        "\n",
        "    return standard_net, attention_net\n",
        "\n",
        "# Execute\n",
        "std_model, attn_model = train_and_compare()"
      ],
      "metadata": {
        "id": "tQDvK6bwbism"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Robustness Testing (The \"Noise\" Challenge)\n",
        "\n",
        "Standard CNNs are notoriously fragile to noise. Biological systems use center-surround attention specifically to filter out environmental \"static.\"\n",
        "\n",
        "Test both models on images with Gaussian Noise or Salt-and-Pepper Noise.\n",
        "- Hypothesis: The Metacognitive model will maintain higher accuracy because the Saliency Gate \"blacks out\" the noise in the background, only letting the high-contrast features through."
      ],
      "metadata": {
        "id": "hMRIl5RDfA9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(batch, noise_level=0.1):\n",
        "    return batch + noise_level * torch.randn_like(batch)\n",
        "\n",
        "# Pending from here\n",
        "# Test both models on noisy_images = add_noise(test_images)"
      ],
      "metadata": {
        "id": "6sR5gJiLfE0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### \"Noise Sensitivity\" Experiment\n",
        "\n",
        "Instead of testing just one noise level, test a range. This will create a \"Decay Curve.\" If your Metacognitive model is superior, its curve will stay \"flat\" longer than the standard model."
      ],
      "metadata": {
        "id": "Y8v0XFmgfq_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_levels = [0.0, 0.1, 0.2, 0.3, 0.5]\n",
        "results = []\n",
        "\n",
        "# Get one batch of data for the stress test\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "for level in noise_levels:\n",
        "    # 1. Add noise\n",
        "    noisy_batch = add_noise(images, noise_level=level)\n",
        "\n",
        "    # 2. Test Standard Model\n",
        "    with torch.no_grad():\n",
        "        # Using the classifier directly as 'standard'\n",
        "        std_logits = standard_net.classifier(noisy_batch)\n",
        "        std_acc = (std_logits.argmax(1) == labels).float().mean().item()\n",
        "\n",
        "        # 3. Test Metacognitive Model\n",
        "        meta_logits, _, _ = attention_net(noisy_batch)\n",
        "        meta_acc = (meta_logits.argmax(1) == labels).float().mean().item()\n",
        "\n",
        "    results.append({\n",
        "        'Noise Level': level,\n",
        "        'Standard Acc': std_acc,\n",
        "        'Metacognitive Acc': meta_acc\n",
        "    })\n",
        "\n",
        "# Display as a Table\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "fjFYXtTAfsRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize \"Attention under Attack\"\n",
        "\n",
        "This is the most interesting part of the notebook. You need to see if the Center-Surround mechanism is successfully \"cleaning\" the image before it hits the classifier.\n",
        "\n",
        "Run this code to compare what the \"Brain\" (the classifier) sees in both models:"
      ],
      "metadata": {
        "id": "NNRZJMItfxTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "level = 0.3\n",
        "noisy_imgs = add_noise(images, noise_level=level)\n",
        "\n",
        "# Get the attention gate's output\n",
        "_, gated_imgs, masks = attention_net(noisy_imgs)\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "axes[0].imshow(noisy_imgs[0].permute(1,2,0).clip(0,1))\n",
        "axes[0].set_title(\"Standard Model Sees This (Noisy)\")\n",
        "\n",
        "axes[1].imshow(masks[0,0].detach(), cmap='gray')\n",
        "axes[1].set_title(\"Metacognitive Attention Map\")\n",
        "\n",
        "axes[2].imshow(gated_imgs[0].detach().permute(1,2,0).clip(0,1))\n",
        "axes[2].set_title(\"Gated Input (Cleaned features)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eyu8mK6gf2Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculate \"Information Sparsity\" (The Efficiency Metric)\n",
        "\n",
        "Since our project is about Metacognitive Attention, we should prove that our model is \"smarter\" because it ignores useless data. We measure this by seeing how \"dark\" the masks are."
      ],
      "metadata": {
        "id": "H1YbFFtlf5JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sparsity(mask):\n",
        "    # Percentage of signal allowed through (0 to 1)\n",
        "    return torch.mean(mask).item()\n",
        "\n",
        "avg_sparsity = calculate_sparsity(masks)\n",
        "print(f\"The model is classifying based on only {avg_sparsity*100:.2f}% of the pixels.\")"
      ],
      "metadata": {
        "id": "fGR6gkF7gCWz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}